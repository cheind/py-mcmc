{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gibbs sampling on distributions with quasi deterministic random variables\n",
    "\n",
    "Assume a binary logical operation represented by a Bayesian Network $P(A,B,O)$ that factores into the distributions $P(A)$, $P(B)$, $P(O|A,B)$. Using Gibbs sampling to approxmimate joint distribution $P(A,B)$, we need to define the resampling distributions \n",
    "\n",
    "\\begin{align}\n",
    "P(A|mb(A)) &= 1/Z P(A)P(O|A,B) \\\\\n",
    "P(B|mb(B)) &= 1/Z P(B)P(O|A,B) \\\\\n",
    "P(O|mb(O)) &= 1/Z P(O|A,B)\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "If the operation $O$ is deterministic on the inputs $A$ and $B$ then Gibbs sampling might get stuck in the initial sample configuration as can be seen in the following. Assume the distributions to be given by\n",
    "\n",
    "| A |P(A)|\n",
    "| --|:---:|\n",
    "| 0 | 0.5 |\n",
    "| 1 | 0.5 |\n",
    "\n",
    "| B | P(B)|\n",
    "| --|:-----:|\n",
    "| 0 | 0.5 |\n",
    "| 1 | 0.5 |\n",
    "\n",
    "| AB| P(O-/A,B)|P(O+/A,B)|\n",
    "| --|:--------:|:-----:|\n",
    "| 00 | 1.0 | 0.0 |\n",
    "| 01 | 0.0 | 1.0 |\n",
    "| 10 | 0.0 | 1.0 |\n",
    "| 11 | 1.0 | 0.0 |\n",
    "\n",
    "which is the classical XOR gate with uniform random inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A = np.array([0.5, 0.5])\n",
    "B = np.array([0.5, 0.5])\n",
    "O_AB = np.array([[[1.0, 0.0], [0.0, 1.0]], [[0.0, 1.0], [1.0, 0.0]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, define the resample distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.,  0.],\n",
       "        [ 0.,  1.]],\n",
       "\n",
       "       [[ 0.,  1.],\n",
       "        [ 1.,  0.]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def redist_A():\n",
    "    A_mb_ABO = A[:, None, None] * O_AB.transpose((2,0,1)) # Read: resample dist of A using variables A,B,O in that order\n",
    "    A_mb_ABO /= A_mb_ABO.sum(axis=0)[None,...]\n",
    "    return A_mb_ABO\n",
    "\n",
    "redist_A()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.,  0.],\n",
       "        [ 0.,  1.]],\n",
       "\n",
       "       [[ 0.,  1.],\n",
       "        [ 1.,  0.]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def redist_B():\n",
    "    B_mb_BAO = B[:, None, None] * O_AB.transpose((2,1,0))\n",
    "    B_mb_BAO /= B_mb_BAO.sum(axis=0)[None,...]\n",
    "    return B_mb_BAO\n",
    "\n",
    "redist_B()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.,  0.],\n",
       "        [ 0.,  1.]],\n",
       "\n",
       "       [[ 0.,  1.],\n",
       "        [ 1.,  0.]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def redist_O():\n",
    "    O_mb_OAB = O_AB\n",
    "    return O_mb_OAB\n",
    "\n",
    "redist_O()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What these resampling distributions indicate is that they are all fully deterministic, despite the marginal distributions of A, B being probabilistic. Hence, the value of any of these variables is fully determined by the known value of the operation and the other second variable. While forward sampling does the right job in this case, Gibbs fails as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw(choices, probs, n=None):\n",
    "    '''Draw samples from probability distributions.\n",
    "    \n",
    "    If n is given, n samples from the same distribution will be drawn.\n",
    "    If n is not given, the number of samples is determined by the number\n",
    "    of columns in probs. Each column then represents a unique probability\n",
    "    distribution to draw a single sample from.\n",
    "    '''\n",
    "    \n",
    "    if n is not None:\n",
    "        u = np.random.uniform(0., 1., size=n)\n",
    "        c = np.cumsum(probs)\n",
    "        return np.asarray(choices)[c.searchsorted(u)]\n",
    "    else:\n",
    "        u = np.random.uniform(0., 1., size=probs.shape[1])\n",
    "        c = np.cumsum(probs, axis=0)\n",
    "        return np.asarray([c[:,i].searchsorted(u[i]) for i in range(probs.shape[1])])\n",
    "\n",
    "# self test for draw\n",
    "assert np.isclose(draw([0, 1], [0.3, 0.7], 10000).sum()/10000, 0.7, atol=1e-2)\n",
    "assert np.isclose(draw([0, 1], np.tile([[0.3], [0.7]], 10000)).sum()/10000, 0.7, atol=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.24762,  0.25254],\n",
       "       [ 0.25157,  0.24827]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def forward_sample(n=1000):\n",
    "    '''Approximates P(A,B) using forward sampling.'''\n",
    "    \n",
    "    # Draw samples through forward sampling\n",
    "    samples = np.zeros((n, 3), dtype=np.int32)\n",
    "    samples[:, 0] = draw([0, 1], A, n=n)\n",
    "    samples[:, 1] = draw([0, 1], B, n=n)\n",
    "    samples[:, 2] = draw([0, 1], O_AB[:, samples[:, 0], samples[:, 1]])\n",
    "    \n",
    "    # Approximate P(A,B) by counting\n",
    "    return np.array([\n",
    "        [((samples[:, 0] == 0) & (samples[:, 1] == 0)).sum(), ((samples[:, 0] == 0) & (samples[:, 1] == 1)).sum()],\n",
    "        [((samples[:, 0] == 1) & (samples[:, 1] == 0)).sum(), ((samples[:, 0] == 1) & (samples[:, 1] == 1)).sum()]]) / n\n",
    "\n",
    "np.array([forward_sample(n=1000) for _ in range(100)]).mean(axis=0) # 100 experiments, each 1000 samples, mean plotted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward sampling produces expected results, each combination of A,B occurs equally likely. Exceptional: not looking at children is a benefit. Whereas Gibbs sampling is stuck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.],\n",
       "       [ 0.,  0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.],\n",
       "       [ 0.,  0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.],\n",
       "       [ 1.,  0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.],\n",
       "       [ 0.,  1.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def gibbs_sample(n=1000, burn_in=100, dist=1, start=None):\n",
    "    choices = [0, 1]\n",
    "    \n",
    "    A_mb_ABO = redist_A()\n",
    "    B_mb_BAO = redist_B()\n",
    "    O_mb_OAB = redist_O()\n",
    "    \n",
    "    resample = [\n",
    "        lambda x: draw(choices, A_mb_ABO[:, x[1], x[2]], n=1), # A\n",
    "        lambda x: draw(choices, B_mb_BAO[:, x[0], x[2]], n=1), # B\n",
    "        lambda x: draw(choices, O_mb_OAB[:, x[0], x[1]], n=1), # O\n",
    "    ]\n",
    "    \n",
    "    def gibbs_update(x, c):\n",
    "        x[c] = resample[c](x)\n",
    "        return x\n",
    "    \n",
    "    # Which vars: A, B, C, A, ...\n",
    "    seq_ids = np.zeros(n*dist+burn_in, dtype=np.int32)\n",
    "    seq_ids[1::3] = 1\n",
    "    seq_ids[2::3] = 2\n",
    "    \n",
    "    if start is None:\n",
    "        x = np.array([0, 1, 1], dtype=np.int32) # Start vec(A,B,C)\n",
    "    else:\n",
    "        x = np.asarray(start)\n",
    "    \n",
    "    idx = 0\n",
    "    def step():\n",
    "        nonlocal idx\n",
    "        v = seq_ids[idx]\n",
    "        x[v] = resample[v](x)\n",
    "        idx += 1\n",
    "        return x\n",
    "\n",
    "    def advance(n):\n",
    "        for _ in range(n):\n",
    "            step()\n",
    "\n",
    "    # burn-in\n",
    "    advance(burn_in)\n",
    "    \n",
    "    # sample\n",
    "    samples = np.empty((n, 3))\n",
    "    for i in range(n):\n",
    "        samples[i] = step()\n",
    "        advance(dist-1)\n",
    "                \n",
    "    # Approximate P(A,B) by counting\n",
    "    return np.array([\n",
    "        [((samples[:, 0] == 0) & (samples[:, 1] == 0)).sum(), ((samples[:, 0] == 0) & (samples[:, 1] == 1)).sum()],\n",
    "        [((samples[:, 0] == 1) & (samples[:, 1] == 0)).sum(), ((samples[:, 0] == 1) & (samples[:, 1] == 1)).sum()]]) / n\n",
    "        \n",
    "# No matter which start point we always produce the same samples, so P(A,B) for all particular start possibilities\n",
    "from IPython.display import display\n",
    "display(gibbs_sample(burn_in=10, dist=10, n=1000, start=[0,0,0]))\n",
    "display(gibbs_sample(burn_in=10, dist=10, n=1000, start=[0,1,1]))\n",
    "display(gibbs_sample(burn_in=10, dist=10, n=1000, start=[1,0,1]))\n",
    "display(gibbs_sample(burn_in=10, dist=10, n=1000, start=[1,1,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No matter what's the starting condition of the Markov chain, we remain in that state forever.\n",
    "\n",
    "By making $P(O|A,B)$ quasi-determinstic we can slowly see Gibbs sampling doing the right thing as far as $P(A,B)$ is concerned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.215,  0.234],\n",
       "       [ 0.263,  0.288]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "O_AB[:] = np.array([[[0.99, 0.01], [0.01, 0.99]], [[0.01, 0.99], [0.99, 0.01]]])\n",
    "display(gibbs_sample(burn_in=100, dist=10, n=1000, start=[0,0,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bit more.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.236,  0.258],\n",
       "       [ 0.242,  0.264]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "O_AB[:] = np.array([[[0.9, 0.1], [0.1, 0.9]], [[0.1, 0.9], [0.9, 0.1]]])\n",
    "display(gibbs_sample(burn_in=100, dist=10, n=1000, start=[0,0,0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
